{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L Layer Deep Neural Network From Scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNSCrvq23EJRS8pQBcqKPEQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prd-dahal/AI_Projects/blob/master/L_Layer_Deep_Neural_Network_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7EeZQC-jvCu",
        "colab_type": "text"
      },
      "source": [
        "## **L Layer Deep Neural Network without Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUaXqC-TljZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dependencies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBL_pwONna_q",
        "colab_type": "code",
        "outputId": "9c8732f2-4c1d-49b7-a70a-837ad25e8128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Main Class \n",
        "\n",
        "class NeuralNetwork:\n",
        "  #initialize the class with hyperparameters. Here L is the depth and N is the array \n",
        "  #that contains the no of node in each Depth\n",
        "  def __init__(self, alpha=0.01, num_iter = 25, verbose='False', threshold=0.0001, L=2, N=[4,1]):\n",
        "    self.alpha = alpha\n",
        "    self.num_iter = num_iter\n",
        "    self.verbose = verbose \n",
        "    self.threshold = threshold \n",
        "    self.L = L\n",
        "    self.N = N\n",
        "    self.nx = 0\n",
        "    self.m = 0\n",
        "    self.W = {}\n",
        "    self.B = {}\n",
        "\n",
        "    #variable for forward propagation\n",
        "    self.Z = {}\n",
        "    self.A = {}\n",
        "\n",
        "    #variable for backward propagation\n",
        "    self.dZ = {}\n",
        "    self.dW = {}\n",
        "    self.dB = {}\n",
        "    self.dA = {}\n",
        "\n",
        "  def __sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "   \n",
        "  #relu activation\n",
        "  def __relu(self,Z):\n",
        "    return np.maximum(0,Z)\n",
        "  \n",
        "  #relu derivative activation\n",
        "  def __reluDerivative(self,Z):\n",
        "       Z[Z<=0] = 0\n",
        "       Z[Z>0] = 1\n",
        "       return Z\n",
        "  \n",
        "  #predict the value for given input x\n",
        "  def predict(self,x):\n",
        "    A = {}\n",
        "    A[0] = x\n",
        "    Z = {}\n",
        "    for i in range(1,self.L+1):\n",
        "        if(i<(self.L)):\n",
        "          Z[i] = np.dot(self.W[i], A[i-1])+ self.B[i]\n",
        "          A[i] = self.__relu(Z[i])\n",
        "        else:\n",
        "          Z[i] = np.dot(self.W[i],A[i-1])+self.B[i]\n",
        "          A[i] = self.__sigmoid(Z[i])\n",
        "    return A[self.L].mean()\n",
        "  \n",
        "  #initialize weights   \n",
        "  def __weightInitialize(self):\n",
        "    self.N.insert(0,self.nx)\n",
        "    \n",
        "    for i in range(1,self.L+1):\n",
        "      self.W[i] = np.random.randn(self.N[i],self.N[i-1])\n",
        "      self.B[i] = np.random.randn(self.N[i],1)\n",
        "    \n",
        "  def fit(self,X,y):\n",
        "    self.nx = X.shape[0]\n",
        "    self.m = X.shape[1]\n",
        "\n",
        "    #initialize all the weights and bias\n",
        "    self.__weightInitialize()\n",
        "    self.A[0] = X\n",
        "    \n",
        "    #epoch start from here\n",
        "    for t in range(self.num_iter):\n",
        "\n",
        "      ##START OF FORWARD PROPAGATION\n",
        "      for i in range(1,self.L+1):\n",
        "        if(i<(self.L)):\n",
        "          self.Z[i] = np.dot(self.W[i], self.A[i-1])+ self.B[i]\n",
        "          self.A[i] = self.__relu(self.Z[i])\n",
        "        else:\n",
        "          self.Z[i] = np.dot(self.W[i],self.A[i-1])+self.B[i]\n",
        "          self.A[i] = self.__sigmoid(self.Z[i])  \n",
        "      ##END OF FORWARD PROPAGATION\n",
        "      \n",
        "      #ERROR CALCULATION\n",
        "      j = - (y * np.log(self.A[self.L]) + (1-y) * np.log(1-self.A[self.L])) #error calculation\n",
        "      J = j.mean()\n",
        "      if(self.verbose==True):\n",
        "        print(\"The error at this iteration {} is {}\".format(t,J))\n",
        "      if(J<self.threshold):\n",
        "        break\n",
        "\n",
        "      ##START OF BACKWARD PROPAGATION\n",
        "      self.dZ[self.L] = self.A[self.L] - y\n",
        "      self.dW[self.L] = (1/self.m) * np.dot(self.dZ[self.L],self.A[self.L].T)\n",
        "      self.dB[self.L] = (1/self.m) * np.sum(self.dZ[self.L],axis=1,keepdims=True)\n",
        "      self.dA[self.L-1] = np.dot(self.W[self.L].T, self.dZ[self.L]) \n",
        "      for i in range(self.L-1,0,-1):\n",
        "        self.dZ[i] = self.dA[i] * self.__reluDerivative(self.Z[i])\n",
        "        self.dW[i] = (1/self.m) * np.dot(self.dZ[i],self.A[i-1].T)\n",
        "        self.dB[i] = (1/self.m) * np.sum(self.dZ[i],axis=1, keepdims=True)\n",
        "        self.dA[i-1] = np.dot(self.W[i].T,self.dZ[i])\n",
        "      ##END OF BACKWARD PROPAGATION\n",
        "\n",
        "      ##WEIGHT UPDATE START\n",
        "      for i in range(1,self.L+1):\n",
        "        self.W[i] = self.W[i] - self.alpha * self.dW[i]\n",
        "        self.B[i] = self.B[i] - self.alpha * self.dB[i]\n",
        "      ## WEIGHT UPDATE ENDS     \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  X = np.array([[1,2,3],\n",
        "                [4,5,6],\n",
        "                [8,9,10],\n",
        "                [11,12,13]])\n",
        "  X = X.T\n",
        "  y = np.array([1,0,1,1])\n",
        "\n",
        "  nn = NeuralNetwork(alpha=0.01, verbose=True, num_iter=500, threshold = 0.4, L=3, N=[4,3,1])\n",
        "  nn.fit(X,y)\n",
        "  print('The predicted value is::{}'.format(nn.predict(np.array([10,12,23]))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The error at this iteration 0 is 1.151056066810515\n",
            "The error at this iteration 1 is 1.14841354795246\n",
            "The error at this iteration 2 is 1.145780561019504\n",
            "The error at this iteration 3 is 1.1431570973099285\n",
            "The error at this iteration 4 is 1.140543147899793\n",
            "The error at this iteration 5 is 1.137938703644028\n",
            "The error at this iteration 6 is 1.1353437551775487\n",
            "The error at this iteration 7 is 1.1327582929163893\n",
            "The error at this iteration 8 is 1.130182307058853\n",
            "The error at this iteration 9 is 1.1276157875866846\n",
            "The error at this iteration 10 is 1.125058724266259\n",
            "The error at this iteration 11 is 1.1225111066497893\n",
            "The error at this iteration 12 is 1.1199729240765532\n",
            "The error at this iteration 13 is 1.1174441656741367\n",
            "The error at this iteration 14 is 1.1149248203596942\n",
            "The error at this iteration 15 is 1.1124148768412279\n",
            "The error at this iteration 16 is 1.1099143236188829\n",
            "The error at this iteration 17 is 1.1074231489862596\n",
            "The error at this iteration 18 is 1.1049413410317408\n",
            "The error at this iteration 19 is 1.1024688876398379\n",
            "The error at this iteration 20 is 1.1000057764925497\n",
            "The error at this iteration 21 is 1.0975519950707389\n",
            "The error at this iteration 22 is 1.0951075306555225\n",
            "The error at this iteration 23 is 1.0926723703296786\n",
            "The error at this iteration 24 is 1.090246500979065\n",
            "The error at this iteration 25 is 1.0878299092940562\n",
            "The error at this iteration 26 is 1.08542258177099\n",
            "The error at this iteration 27 is 1.0830245047136315\n",
            "The error at this iteration 28 is 1.0806356642346477\n",
            "The error at this iteration 29 is 1.0782560462570983\n",
            "The error at this iteration 30 is 1.0758856365159342\n",
            "The error at this iteration 31 is 1.0735244205595142\n",
            "The error at this iteration 32 is 1.0711723837511324\n",
            "The error at this iteration 33 is 1.0688295112705517\n",
            "The error at this iteration 34 is 1.0664957881155588\n",
            "The error at this iteration 35 is 1.0641711991035216\n",
            "The error at this iteration 36 is 1.0618557288729633\n",
            "The error at this iteration 37 is 1.0595493618851433\n",
            "The error at this iteration 38 is 1.0572520824256508\n",
            "The error at this iteration 39 is 1.0549638746060088\n",
            "The error at this iteration 40 is 1.0526847223652833\n",
            "The error at this iteration 41 is 1.0504146094717093\n",
            "The error at this iteration 42 is 1.0481535195243188\n",
            "The error at this iteration 43 is 1.045901435954582\n",
            "The error at this iteration 44 is 1.0436583420280547\n",
            "The error at this iteration 45 is 1.0414242208460356\n",
            "The error at this iteration 46 is 1.0391990553472286\n",
            "The error at this iteration 47 is 1.0369828283094171\n",
            "The error at this iteration 48 is 1.0347755223511406\n",
            "The error at this iteration 49 is 1.0325771199333815\n",
            "The error at this iteration 50 is 1.0303876033612573\n",
            "The error at this iteration 51 is 1.0282069547857198\n",
            "The error at this iteration 52 is 1.026035156205258\n",
            "The error at this iteration 53 is 1.0238721894676115\n",
            "The error at this iteration 54 is 1.021718036271483\n",
            "The error at this iteration 55 is 1.019572678168261\n",
            "The error at this iteration 56 is 1.017436096563745\n",
            "The error at this iteration 57 is 1.015308272719876\n",
            "The error at this iteration 58 is 1.0131891877564692\n",
            "The error at this iteration 59 is 1.0110788226529541\n",
            "The error at this iteration 60 is 1.0089771582501155\n",
            "The error at this iteration 61 is 1.0068841752518378\n",
            "The error at this iteration 62 is 1.0047998542268544\n",
            "The error at this iteration 63 is 1.0027241756104974\n",
            "The error at this iteration 64 is 1.000657119706452\n",
            "The error at this iteration 65 is 0.99859866668851\n",
            "The error at this iteration 66 is 0.9965487966023299\n",
            "The error at this iteration 67 is 0.9945074893671928\n",
            "The error at this iteration 68 is 0.992474724777765\n",
            "The error at this iteration 69 is 0.9904504825058577\n",
            "The error at this iteration 70 is 0.9884347421021893\n",
            "The error at this iteration 71 is 0.9864274829981491\n",
            "The error at this iteration 72 is 0.9844286845075576\n",
            "The error at this iteration 73 is 0.9824383258284322\n",
            "The error at this iteration 74 is 0.9804563860447466\n",
            "The error at this iteration 75 is 0.9784828441281949\n",
            "The error at this iteration 76 is 0.9765176789399511\n",
            "The error at this iteration 77 is 0.9745608692324299\n",
            "The error at this iteration 78 is 0.9726123936510445\n",
            "The error at this iteration 79 is 0.9706722307359645\n",
            "The error at this iteration 80 is 0.9687403589238702\n",
            "The error at this iteration 81 is 0.9668167565497062\n",
            "The error at this iteration 82 is 0.9649014018484309\n",
            "The error at this iteration 83 is 0.9629942729567671\n",
            "The error at this iteration 84 is 0.9610953479149442\n",
            "The error at this iteration 85 is 0.9592046046684436\n",
            "The error at this iteration 86 is 0.9573220210697353\n",
            "The error at this iteration 87 is 0.9554475748800149\n",
            "The error at this iteration 88 is 0.9535812437709348\n",
            "The error at this iteration 89 is 0.9517230053263319\n",
            "The error at this iteration 90 is 0.949872837043952\n",
            "The error at this iteration 91 is 0.9480307163371681\n",
            "The error at this iteration 92 is 0.9461966205366967\n",
            "The error at this iteration 93 is 0.9443705268923055\n",
            "The error at this iteration 94 is 0.9425524125745206\n",
            "The error at this iteration 95 is 0.9407422546763247\n",
            "The error at this iteration 96 is 0.9389400302148515\n",
            "The error at this iteration 97 is 0.9371457161330752\n",
            "The error at this iteration 98 is 0.9353592893014921\n",
            "The error at this iteration 99 is 0.9335807265197987\n",
            "The error at this iteration 100 is 0.931810004518562\n",
            "The error at this iteration 101 is 0.930047099960883\n",
            "The error at this iteration 102 is 0.9282919894440558\n",
            "The error at this iteration 103 is 0.9265446495012166\n",
            "The error at this iteration 104 is 0.9248050566029911\n",
            "The error at this iteration 105 is 0.9230731871591278\n",
            "The error at this iteration 106 is 0.9213490175201304\n",
            "The error at this iteration 107 is 0.9196325239788796\n",
            "The error at this iteration 108 is 0.917923682772247\n",
            "The error at this iteration 109 is 0.9162224700827035\n",
            "The error at this iteration 110 is 0.9145288620399178\n",
            "The error at this iteration 111 is 0.9128428347223476\n",
            "The error at this iteration 112 is 0.9111643641588231\n",
            "The error at this iteration 113 is 0.9094934263301214\n",
            "The error at this iteration 114 is 0.9078299971705328\n",
            "The error at this iteration 115 is 0.906174052569418\n",
            "The error at this iteration 116 is 0.904525568372758\n",
            "The error at this iteration 117 is 0.9028845203846938\n",
            "The error at this iteration 118 is 0.901250884369056\n",
            "The error at this iteration 119 is 0.8996246360508896\n",
            "The error at this iteration 120 is 0.8980057511179642\n",
            "The error at this iteration 121 is 0.8963942052222783\n",
            "The error at this iteration 122 is 0.8947899739815537\n",
            "The error at this iteration 123 is 0.8931930329807183\n",
            "The error at this iteration 124 is 0.8916033577733827\n",
            "The error at this iteration 125 is 0.890020923883303\n",
            "The error at this iteration 126 is 0.8884457068058373\n",
            "The error at this iteration 127 is 0.8868776820093898\n",
            "The error at this iteration 128 is 0.8853168249368455\n",
            "The error at this iteration 129 is 0.8837631110069951\n",
            "The error at this iteration 130 is 0.8822165156159499\n",
            "The error at this iteration 131 is 0.8806770141385447\n",
            "The error at this iteration 132 is 0.8791445819297319\n",
            "The error at this iteration 133 is 0.8776191943259652\n",
            "The error at this iteration 134 is 0.8761008266465706\n",
            "The error at this iteration 135 is 0.8745894541951093\n",
            "The error at this iteration 136 is 0.8730850522607279\n",
            "The error at this iteration 137 is 0.8715875961194992\n",
            "The error at this iteration 138 is 0.8700970610357516\n",
            "The error at this iteration 139 is 0.8686134222633866\n",
            "The error at this iteration 140 is 0.8671366550471874\n",
            "The error at this iteration 141 is 0.8656667346241136\n",
            "The error at this iteration 142 is 0.8642036362245877\n",
            "The error at this iteration 143 is 0.862747335073769\n",
            "The error at this iteration 144 is 0.8612978063928155\n",
            "The error at this iteration 145 is 0.8598550254001367\n",
            "The error at this iteration 146 is 0.858418967312633\n",
            "The error at this iteration 147 is 0.8569896073469241\n",
            "The error at this iteration 148 is 0.8555669207205683\n",
            "The error at this iteration 149 is 0.8541508826532669\n",
            "The error at this iteration 150 is 0.85274146836806\n",
            "The error at this iteration 151 is 0.8513386530925086\n",
            "The error at this iteration 152 is 0.8499424120598673\n",
            "The error at this iteration 153 is 0.8485527205102443\n",
            "The error at this iteration 154 is 0.8471695536917494\n",
            "The error at this iteration 155 is 0.8457928868616318\n",
            "The error at this iteration 156 is 0.8444226952874048\n",
            "The error at this iteration 157 is 0.8430589542479602\n",
            "The error at this iteration 158 is 0.841701639034671\n",
            "The error at this iteration 159 is 0.8403507249524809\n",
            "The error at this iteration 160 is 0.8390061873209839\n",
            "The error at this iteration 161 is 0.837668001475492\n",
            "The error at this iteration 162 is 0.8363361427680902\n",
            "The error at this iteration 163 is 0.8350105865686811\n",
            "The error at this iteration 164 is 0.8336913082660169\n",
            "The error at this iteration 165 is 0.8323782832687209\n",
            "The error at this iteration 166 is 0.8310714870062959\n",
            "The error at this iteration 167 is 0.8297708949301222\n",
            "The error at this iteration 168 is 0.8284764825144437\n",
            "The error at this iteration 169 is 0.8271882252573415\n",
            "The error at this iteration 170 is 0.8259060986816971\n",
            "The error at this iteration 171 is 0.8246300783361439\n",
            "The error at this iteration 172 is 0.8233601397960062\n",
            "The error at this iteration 173 is 0.8220962586642271\n",
            "The error at this iteration 174 is 0.8208384105722861\n",
            "The error at this iteration 175 is 0.8195865711811028\n",
            "The error at this iteration 176 is 0.8183407161819315\n",
            "The error at this iteration 177 is 0.8171008212972428\n",
            "The error at this iteration 178 is 0.8158668622815937\n",
            "The error at this iteration 179 is 0.8146388149224887\n",
            "The error at this iteration 180 is 0.8134166550412258\n",
            "The error at this iteration 181 is 0.8122003584937342\n",
            "The error at this iteration 182 is 0.8109899011713992\n",
            "The error at this iteration 183 is 0.8097852590018764\n",
            "The error at this iteration 184 is 0.8085864079498943\n",
            "The error at this iteration 185 is 0.8073933240180463\n",
            "The error at this iteration 186 is 0.8062059832475706\n",
            "The error at this iteration 187 is 0.8050243617191206\n",
            "The error at this iteration 188 is 0.8038484355535223\n",
            "The error at this iteration 189 is 0.8026781809125214\n",
            "The error at this iteration 190 is 0.8015135739995207\n",
            "The error at this iteration 191 is 0.8003545910603048\n",
            "The error at this iteration 192 is 0.7992012083837547\n",
            "The error at this iteration 193 is 0.7980534023025514\n",
            "The error at this iteration 194 is 0.7969111491938687\n",
            "The error at this iteration 195 is 0.7957744254800562\n",
            "The error at this iteration 196 is 0.7946432076293096\n",
            "The error at this iteration 197 is 0.7935174721563328\n",
            "The error at this iteration 198 is 0.7923971956229869\n",
            "The error at this iteration 199 is 0.7912823546389307\n",
            "The error at this iteration 200 is 0.7901729258622505\n",
            "The error at this iteration 201 is 0.7890688860000776\n",
            "The error at this iteration 202 is 0.7879702118091987\n",
            "The error at this iteration 203 is 0.7868768800966525\n",
            "The error at this iteration 204 is 0.7857888677203186\n",
            "The error at this iteration 205 is 0.7847061515894953\n",
            "The error at this iteration 206 is 0.7836287086654676\n",
            "The error at this iteration 207 is 0.7825565159620644\n",
            "The error at this iteration 208 is 0.7814895505462062\n",
            "The error at this iteration 209 is 0.780427789538444\n",
            "The error at this iteration 210 is 0.7793712101134862\n",
            "The error at this iteration 211 is 0.7783197895007172\n",
            "The error at this iteration 212 is 0.7772735049847054\n",
            "The error at this iteration 213 is 0.7762323339057033\n",
            "The error at this iteration 214 is 0.7751962536601356\n",
            "The error at this iteration 215 is 0.7741652417010787\n",
            "The error at this iteration 216 is 0.7731392755387321\n",
            "The error at this iteration 217 is 0.7721183327408787\n",
            "The error at this iteration 218 is 0.7711023909333358\n",
            "The error at this iteration 219 is 0.7700914278003985\n",
            "The error at this iteration 220 is 0.769085421085272\n",
            "The error at this iteration 221 is 0.7680843485904958\n",
            "The error at this iteration 222 is 0.7670881881783591\n",
            "The error at this iteration 223 is 0.7660969177713065\n",
            "The error at this iteration 224 is 0.7651105153523351\n",
            "The error at this iteration 225 is 0.7641289589653826\n",
            "The error at this iteration 226 is 0.763152226715708\n",
            "The error at this iteration 227 is 0.7621802967702608\n",
            "The error at this iteration 228 is 0.7612131473580448\n",
            "The error at this iteration 229 is 0.7602507567704707\n",
            "The error at this iteration 230 is 0.7592931033617027\n",
            "The error at this iteration 231 is 0.7583401655489935\n",
            "The error at this iteration 232 is 0.7573919218130155\n",
            "The error at this iteration 233 is 0.7564483506981782\n",
            "The error at this iteration 234 is 0.7555094308129433\n",
            "The error at this iteration 235 is 0.7545751408301264\n",
            "The error at this iteration 236 is 0.7536454594871949\n",
            "The error at this iteration 237 is 0.7527203655865553\n",
            "The error at this iteration 238 is 0.7517998379958329\n",
            "The error at this iteration 239 is 0.7508838556481465\n",
            "The error at this iteration 240 is 0.7499723975423703\n",
            "The error at this iteration 241 is 0.7490654427433934\n",
            "The error at this iteration 242 is 0.7481629703823676\n",
            "The error at this iteration 243 is 0.7472649596569515\n",
            "The error at this iteration 244 is 0.7463713898315433\n",
            "The error at this iteration 245 is 0.7454822402375086\n",
            "The error at this iteration 246 is 0.7445974902734016\n",
            "The error at this iteration 247 is 0.7437171194051763\n",
            "The error at this iteration 248 is 0.7428411071663933\n",
            "The error at this iteration 249 is 0.7419694331584181\n",
            "The error at this iteration 250 is 0.7411020770506134\n",
            "The error at this iteration 251 is 0.7402390185805224\n",
            "The error at this iteration 252 is 0.7393802375540492\n",
            "The error at this iteration 253 is 0.7385257138456274\n",
            "The error at this iteration 254 is 0.737675427398387\n",
            "The error at this iteration 255 is 0.736829358224311\n",
            "The error at this iteration 256 is 0.7359874864043865\n",
            "The error at this iteration 257 is 0.7351497920887514\n",
            "The error at this iteration 258 is 0.7343162554968314\n",
            "The error at this iteration 259 is 0.7334868569174728\n",
            "The error at this iteration 260 is 0.7326615767090687\n",
            "The error at this iteration 261 is 0.7318403952996789\n",
            "The error at this iteration 262 is 0.7310232931871434\n",
            "The error at this iteration 263 is 0.7302102509391899\n",
            "The error at this iteration 264 is 0.7294012491935367\n",
            "The error at this iteration 265 is 0.7285962686579873\n",
            "The error at this iteration 266 is 0.7277952901105218\n",
            "The error at this iteration 267 is 0.7269982943993802\n",
            "The error at this iteration 268 is 0.7262052624431415\n",
            "The error at this iteration 269 is 0.725416175230797\n",
            "The error at this iteration 270 is 0.7246310138218178\n",
            "The error at this iteration 271 is 0.7238497593462168\n",
            "The error at this iteration 272 is 0.7230723930046056\n",
            "The error at this iteration 273 is 0.7222988960682455\n",
            "The error at this iteration 274 is 0.7215292498790945\n",
            "The error at this iteration 275 is 0.7207634358498473\n",
            "The error at this iteration 276 is 0.7200014354639714\n",
            "The error at this iteration 277 is 0.7192432302757392\n",
            "The error at this iteration 278 is 0.7184888019102516\n",
            "The error at this iteration 279 is 0.717738132063461\n",
            "The error at this iteration 280 is 0.7169912025021853\n",
            "The error at this iteration 281 is 0.7162479950641216\n",
            "The error at this iteration 282 is 0.7155084916578507\n",
            "The error at this iteration 283 is 0.7147726742628397\n",
            "The error at this iteration 284 is 0.7140405249294403\n",
            "The error at this iteration 285 is 0.7133120257788794\n",
            "The error at this iteration 286 is 0.7125871590032495\n",
            "The error at this iteration 287 is 0.711865906865492\n",
            "The error at this iteration 288 is 0.7111482516993761\n",
            "The error at this iteration 289 is 0.7104341759094753\n",
            "The error at this iteration 290 is 0.7097236619711375\n",
            "The error at this iteration 291 is 0.7090166924304525\n",
            "The error at this iteration 292 is 0.708313249904215\n",
            "The error at this iteration 293 is 0.7076133170798824\n",
            "The error at this iteration 294 is 0.7069168767155322\n",
            "The error at this iteration 295 is 0.7062239116398095\n",
            "The error at this iteration 296 is 0.705534404751877\n",
            "The error at this iteration 297 is 0.7048483390213575\n",
            "The error at this iteration 298 is 0.7041656974882726\n",
            "The error at this iteration 299 is 0.7034864632629798\n",
            "The error at this iteration 300 is 0.7028106195261047\n",
            "The error at this iteration 301 is 0.7021381495284693\n",
            "The error at this iteration 302 is 0.7014690365910178\n",
            "The error at this iteration 303 is 0.7008032641047381\n",
            "The error at this iteration 304 is 0.70014081553058\n",
            "The error at this iteration 305 is 0.6994816743993718\n",
            "The error at this iteration 306 is 0.69882582431173\n",
            "The error at this iteration 307 is 0.6981732489379693\n",
            "The error at this iteration 308 is 0.6975239320180087\n",
            "The error at this iteration 309 is 0.6968778573612715\n",
            "The error at this iteration 310 is 0.6962350088465872\n",
            "The error at this iteration 311 is 0.6955953704220867\n",
            "The error at this iteration 312 is 0.6949589261050948\n",
            "The error at this iteration 313 is 0.6943256599820224\n",
            "The error at this iteration 314 is 0.6936955562082526\n",
            "The error at this iteration 315 is 0.6930685990080261\n",
            "The error at this iteration 316 is 0.6924447726743229\n",
            "The error at this iteration 317 is 0.6918240615687425\n",
            "The error at this iteration 318 is 0.6912064501213799\n",
            "The error at this iteration 319 is 0.6905919228306989\n",
            "The error at this iteration 320 is 0.6899804642634055\n",
            "The error at this iteration 321 is 0.6893720590543152\n",
            "The error at this iteration 322 is 0.6887666919062202\n",
            "The error at this iteration 323 is 0.6881643475897541\n",
            "The error at this iteration 324 is 0.6875650109432525\n",
            "The error at this iteration 325 is 0.6869686668726134\n",
            "The error at this iteration 326 is 0.6863753003511539\n",
            "The error at this iteration 327 is 0.6857848964194655\n",
            "The error at this iteration 328 is 0.6851974401852662\n",
            "The error at this iteration 329 is 0.684612916823252\n",
            "The error at this iteration 330 is 0.6840313115749446\n",
            "The error at this iteration 331 is 0.6834526097485387\n",
            "The error at this iteration 332 is 0.6828767967187455\n",
            "The error at this iteration 333 is 0.682303857926636\n",
            "The error at this iteration 334 is 0.6817337788794808\n",
            "The error at this iteration 335 is 0.6811665451505899\n",
            "The error at this iteration 336 is 0.6806021423791484\n",
            "The error at this iteration 337 is 0.680040556270052\n",
            "The error at this iteration 338 is 0.6794817725937403\n",
            "The error at this iteration 339 is 0.6789257771860281\n",
            "The error at this iteration 340 is 0.6783725559479357\n",
            "The error at this iteration 341 is 0.6778220948455163\n",
            "The error at this iteration 342 is 0.6772743799096835\n",
            "The error at this iteration 343 is 0.6767293972360361\n",
            "The error at this iteration 344 is 0.6761871329846807\n",
            "The error at this iteration 345 is 0.6756475733800553\n",
            "The error at this iteration 346 is 0.6751107047107484\n",
            "The error at this iteration 347 is 0.6745765133293187\n",
            "The error at this iteration 348 is 0.6740449856521126\n",
            "The error at this iteration 349 is 0.6735161081590811\n",
            "The error at this iteration 350 is 0.672989867393595\n",
            "The error at this iteration 351 is 0.6724662499622566\n",
            "The error at this iteration 352 is 0.6719452425347153\n",
            "The error at this iteration 353 is 0.6714268318434768\n",
            "The error at this iteration 354 is 0.6709110046837143\n",
            "The error at this iteration 355 is 0.670397747913076\n",
            "The error at this iteration 356 is 0.6698870484514949\n",
            "The error at this iteration 357 is 0.6693788932809946\n",
            "The error at this iteration 358 is 0.668873269445494\n",
            "The error at this iteration 359 is 0.6683701640506144\n",
            "The error at this iteration 360 is 0.6678695642634811\n",
            "The error at this iteration 361 is 0.6673714573125269\n",
            "The error at this iteration 362 is 0.6668758304872944\n",
            "The error at this iteration 363 is 0.6663826711382358\n",
            "The error at this iteration 364 is 0.6658919666765136\n",
            "The error at this iteration 365 is 0.6654037045738006\n",
            "The error at this iteration 366 is 0.6649178723620761\n",
            "The error at this iteration 367 is 0.664434457633426\n",
            "The error at this iteration 368 is 0.6639534480398374\n",
            "The error at this iteration 369 is 0.6634748312929964\n",
            "The error at this iteration 370 is 0.6629985951640829\n",
            "The error at this iteration 371 is 0.6625247274835646\n",
            "The error at this iteration 372 is 0.6620532161409922\n",
            "The error at this iteration 373 is 0.6615840490847922\n",
            "The error at this iteration 374 is 0.6611172143220594\n",
            "The error at this iteration 375 is 0.6606526999183508\n",
            "The error at this iteration 376 is 0.6601904939974748\n",
            "The error at this iteration 377 is 0.6597305847412841\n",
            "The error at this iteration 378 is 0.6592729603894663\n",
            "The error at this iteration 379 is 0.6588176092393332\n",
            "The error at this iteration 380 is 0.6583645196456116\n",
            "The error at this iteration 381 is 0.6579136800202318\n",
            "The error at this iteration 382 is 0.6574650788321165\n",
            "The error at this iteration 383 is 0.6570187046069701\n",
            "The error at this iteration 384 is 0.6565745459270658\n",
            "The error at this iteration 385 is 0.6561325914310345\n",
            "The error at this iteration 386 is 0.6556928298136512\n",
            "The error at this iteration 387 is 0.6552552498256227\n",
            "The error at this iteration 388 is 0.6548198402733744\n",
            "The error at this iteration 389 is 0.654386590018837\n",
            "The error at this iteration 390 is 0.6539554879792332\n",
            "The error at this iteration 391 is 0.6535265231268629\n",
            "The error at this iteration 392 is 0.6530996844888901\n",
            "The error at this iteration 393 is 0.6526749611471288\n",
            "The error at this iteration 394 is 0.6522523422378268\n",
            "The error at this iteration 395 is 0.651831816951454\n",
            "The error at this iteration 396 is 0.6514133745324857\n",
            "The error at this iteration 397 is 0.6509970042791884\n",
            "The error at this iteration 398 is 0.6505826955434053\n",
            "The error at this iteration 399 is 0.6501704377303407\n",
            "The error at this iteration 400 is 0.6497602202983455\n",
            "The error at this iteration 401 is 0.6493520327587022\n",
            "The error at this iteration 402 is 0.6489458646754099\n",
            "The error at this iteration 403 is 0.6485417056649687\n",
            "The error at this iteration 404 is 0.6481395453961655\n",
            "The error at this iteration 405 is 0.6477393735898581\n",
            "The error at this iteration 406 is 0.6473411800187612\n",
            "The error at this iteration 407 is 0.646944954507231\n",
            "The error at this iteration 408 is 0.6465506869310496\n",
            "The error at this iteration 409 is 0.6461583672172119\n",
            "The error at this iteration 410 is 0.64576798534371\n",
            "The error at this iteration 411 is 0.645379531339318\n",
            "The error at this iteration 412 is 0.6449929952833793\n",
            "The error at this iteration 413 is 0.6446083673055907\n",
            "The error at this iteration 414 is 0.6442256375857895\n",
            "The error at this iteration 415 is 0.6438447963537386\n",
            "The error at this iteration 416 is 0.6434658338889137\n",
            "The error at this iteration 417 is 0.6430887405202886\n",
            "The error at this iteration 418 is 0.6427135066261236\n",
            "The error at this iteration 419 is 0.6423401226337501\n",
            "The error at this iteration 420 is 0.64196857901936\n",
            "The error at this iteration 421 is 0.6415988663077912\n",
            "The error at this iteration 422 is 0.6412309750723163\n",
            "The error at this iteration 423 is 0.6408648959344306\n",
            "The error at this iteration 424 is 0.6405006195636392\n",
            "The error at this iteration 425 is 0.6401381366772465\n",
            "The error at this iteration 426 is 0.6397774380401446\n",
            "The error at this iteration 427 is 0.6394185144646022\n",
            "The error at this iteration 428 is 0.6390613568100543\n",
            "The error at this iteration 429 is 0.6387059559828914\n",
            "The error at this iteration 430 is 0.6383523029362507\n",
            "The error at this iteration 431 is 0.6380003886698047\n",
            "The error at this iteration 432 is 0.6376502042295543\n",
            "The error at this iteration 433 is 0.6373017407076175\n",
            "The error at this iteration 434 is 0.6369549892420232\n",
            "The error at this iteration 435 is 0.6366099410165013\n",
            "The error at this iteration 436 is 0.6362665872602762\n",
            "The error at this iteration 437 is 0.6359249192478588\n",
            "The error at this iteration 438 is 0.6355849282988405\n",
            "The error at this iteration 439 is 0.6352466057776859\n",
            "The error at this iteration 440 is 0.6349099430935271\n",
            "The error at this iteration 441 is 0.6345749316999586\n",
            "The error at this iteration 442 is 0.6342415630948313\n",
            "The error at this iteration 443 is 0.6339098288200486\n",
            "The error at this iteration 444 is 0.6335797204613624\n",
            "The error at this iteration 445 is 0.6332512296481682\n",
            "The error at this iteration 446 is 0.6329243480533031\n",
            "The error at this iteration 447 is 0.6325990673928424\n",
            "The error at this iteration 448 is 0.632275379425898\n",
            "The error at this iteration 449 is 0.6319532759544152\n",
            "The error at this iteration 450 is 0.6316327488229734\n",
            "The error at this iteration 451 is 0.6313137899185833\n",
            "The error at this iteration 452 is 0.630996391170489\n",
            "The error at this iteration 453 is 0.630680544549966\n",
            "The error at this iteration 454 is 0.6303662420701235\n",
            "The error at this iteration 455 is 0.6300534757857049\n",
            "The error at this iteration 456 is 0.6297422377928906\n",
            "The error at this iteration 457 is 0.6294325202290996\n",
            "The error at this iteration 458 is 0.629124315272793\n",
            "The error at this iteration 459 is 0.6288176151432772\n",
            "The error at this iteration 460 is 0.6285124121005081\n",
            "The error at this iteration 461 is 0.6282086984448966\n",
            "The error at this iteration 462 is 0.627906466517112\n",
            "The error at this iteration 463 is 0.6276057086978898\n",
            "The error at this iteration 464 is 0.627306417407837\n",
            "The error at this iteration 465 is 0.627008585107239\n",
            "The error at this iteration 466 is 0.626712204295868\n",
            "The error at this iteration 467 is 0.6264172675127907\n",
            "The error at this iteration 468 is 0.6261237673361764\n",
            "The error at this iteration 469 is 0.6258316963831078\n",
            "The error at this iteration 470 is 0.6255410473093895\n",
            "The error at this iteration 471 is 0.6252518128093598\n",
            "The error at this iteration 472 is 0.6249639856157012\n",
            "The error at this iteration 473 is 0.6246775584992522\n",
            "The error at this iteration 474 is 0.6243925242688202\n",
            "The error at this iteration 475 is 0.6241088757709938\n",
            "The error at this iteration 476 is 0.6238266058899574\n",
            "The error at this iteration 477 is 0.6235457075473047\n",
            "The error at this iteration 478 is 0.6232661737018539\n",
            "The error at this iteration 479 is 0.6229879973494636\n",
            "The error at this iteration 480 is 0.6227111715228485\n",
            "The error at this iteration 481 is 0.6224356892913965\n",
            "The error at this iteration 482 is 0.622161543760986\n",
            "The error at this iteration 483 is 0.6218887280738042\n",
            "The error at this iteration 484 is 0.6216172354081663\n",
            "The error at this iteration 485 is 0.6213470589783333\n",
            "The error at this iteration 486 is 0.6210781920343346\n",
            "The error at this iteration 487 is 0.6208106278617863\n",
            "The error at this iteration 488 is 0.620544359781714\n",
            "The error at this iteration 489 is 0.6202793811503744\n",
            "The error at this iteration 490 is 0.6200156853590779\n",
            "The error at this iteration 491 is 0.6197532658340121\n",
            "The error at this iteration 492 is 0.619492116036066\n",
            "The error at this iteration 493 is 0.6192322294606541\n",
            "The error at this iteration 494 is 0.6189735996375425\n",
            "The error at this iteration 495 is 0.6187162201306741\n",
            "The error at this iteration 496 is 0.6184600845379957\n",
            "The error at this iteration 497 is 0.6182051864912855\n",
            "The error at this iteration 498 is 0.6179515196559807\n",
            "The error at this iteration 499 is 0.6176990777310067\n",
            "The predicted value is::0.6305037702292258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHYy5U-Bx-YR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-uS9noAj3Q5",
        "colab_type": "text"
      },
      "source": [
        "## **L Layer Deep Neural Network with L2 Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ5EVi8VkFJI",
        "colab_type": "code",
        "outputId": "b2414f83-3d60-42a3-97fd-0c31d886c61f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "#Main Class \n",
        "\n",
        "class NeuralNetworkR:\n",
        "  #initialize the class with hyperparameters. Here L is the depth and N is the array \n",
        "  #that contains the no of node in each Depth\n",
        "  def __init__(self, alpha=0.01, num_iter = 25, verbose='False', threshold=0.0001, L=2, N=[4,1], lambd=1):\n",
        "    self.alpha = alpha\n",
        "    self.num_iter = num_iter\n",
        "    self.verbose = verbose \n",
        "    self.threshold = threshold \n",
        "    self.L = L\n",
        "    self.N = N\n",
        "    self.lambd = lambd\n",
        "    self.nx = 0\n",
        "    self.m = 0\n",
        "    self.W = {}\n",
        "    self.B = {}\n",
        "    self.JArray = list()\n",
        "    self.reg_sum = 0\n",
        "    #variable for forward propagation\n",
        "    self.Z = {}\n",
        "    self.A = {}\n",
        "\n",
        "    #variable for backward propagation\n",
        "    self.dZ = {}\n",
        "    self.dW = {}\n",
        "    self.dB = {}\n",
        "    self.dA = {}\n",
        "\n",
        "  def __sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "   \n",
        "  #relu activation\n",
        "  def __relu(self,Z):\n",
        "    return np.maximum(0,Z)\n",
        "  \n",
        "  #relu derivative activation\n",
        "  def __reluDerivative(self,Z):\n",
        "       Z[Z<=0] = 0\n",
        "       Z[Z>0] = 1\n",
        "       return Z\n",
        "  \n",
        "  #predict the value for given input x\n",
        "  def predict(self,x):\n",
        "    A = {}\n",
        "    A[0] = x\n",
        "    Z = {}\n",
        "    for i in range(1,self.L+1):\n",
        "        if(i<(self.L)):\n",
        "          Z[i] = np.dot(self.W[i], A[i-1])+ self.B[i]\n",
        "          A[i] = self.__relu(Z[i])\n",
        "        else:\n",
        "          Z[i] = np.dot(self.W[i],A[i-1])+self.B[i]\n",
        "          A[i] = self.__sigmoid(Z[i])\n",
        "    return A[self.L].mean()\n",
        "  \n",
        "  #initialize weights   \n",
        "  def __weightInitialize(self):\n",
        "    self.N.insert(0,self.nx)\n",
        "    \n",
        "    for i in range(1,self.L+1):\n",
        "      self.W[i] = np.random.randn(self.N[i],self.N[i-1])\n",
        "      self.B[i] = np.random.randn(self.N[i],1)\n",
        "  \n",
        "  #return array of errors \n",
        "  def array_error(self):\n",
        "    return self.num_iter,self.JArray\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    self.nx = X.shape[0]\n",
        "    self.m = X.shape[1]\n",
        "\n",
        "    #initialize all the weights and bias\n",
        "    self.__weightInitialize()\n",
        "    self.A[0] = X\n",
        "    \n",
        "    #epoch start from here\n",
        "    for t in range(self.num_iter):\n",
        "\n",
        "      ##START OF FORWARD PROPAGATION\n",
        "      for i in range(1,self.L+1):\n",
        "        if(i<(self.L)):\n",
        "          self.Z[i] = np.dot(self.W[i], self.A[i-1])+ self.B[i]\n",
        "          self.A[i] = self.__relu(self.Z[i])\n",
        "        else:\n",
        "          self.Z[i] = np.dot(self.W[i],self.A[i-1])+self.B[i]\n",
        "          self.A[i] = self.__sigmoid(self.Z[i])  \n",
        "      ##END OF FORWARD PROPAGATION\n",
        "      \n",
        "      #ERROR CALCULATION\n",
        "      j = - (y * np.log(self.A[self.L]) + (1-y) * np.log(1-self.A[self.L])) #error calculation\n",
        "      \n",
        "      #calculate regularization factor\n",
        "      for i in range(1,self.L+1):\n",
        "        self.reg_sum = self.reg_sum + np.sum(self.W[i]*self.W[i])\n",
        "        self.reg_sum = (self.lambd / (2* self.m)) * self.reg_sum\n",
        "\n",
        "      J = j.mean() + self.reg_sum\n",
        "      self.JArray.append(J)\n",
        "      if(self.verbose==True):\n",
        "        print(\"The error at this iteration {} is {}\".format(t,J))\n",
        "      if(J<self.threshold):\n",
        "        break\n",
        "\n",
        "      ##START OF BACKWARD PROPAGATION\n",
        "      self.dZ[self.L] = self.A[self.L] - y\n",
        "      self.dW[self.L] = ((1/self.m) * np.dot(self.dZ[self.L],self.A[self.L].T)) + ((self.lambd/self.m)* self.W[self.L])\n",
        "      self.dB[self.L] = (1/self.m) * np.sum(self.dZ[self.L],axis=1,keepdims=True)\n",
        "      self.dA[self.L-1] = np.dot(self.W[self.L].T, self.dZ[self.L]) \n",
        "      for i in range(self.L-1,0,-1):\n",
        "        self.dZ[i] = self.dA[i] * self.__reluDerivative(self.Z[i])\n",
        "        self.dW[i] = (1/self.m) * np.dot(self.dZ[i],self.A[i-1].T) + ((self.lambd/self.m)* self.W[i])\n",
        "        self.dB[i] = (1/self.m) * np.sum(self.dZ[i],axis=1, keepdims=True)\n",
        "        self.dA[i-1] = np.dot(self.W[i].T,self.dZ[i])\n",
        "      ##END OF BACKWARD PROPAGATION\n",
        "\n",
        "      ##WEIGHT UPDATE START\n",
        "      for i in range(1,self.L+1):\n",
        "        self.W[i] = self.W[i] - self.alpha * self.dW[i]\n",
        "        self.B[i] = self.B[i] - self.alpha * self.dB[i]\n",
        "      ## WEIGHT UPDATE ENDS     \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  X = np.array([[1,2,3],\n",
        "                [4,5,6],\n",
        "                [8,9,10],\n",
        "                [11,12,13]])\n",
        "  X = X.T\n",
        "  y = np.array([1,0,1,1])\n",
        "\n",
        "  nnr = NeuralNetworkR(alpha=0.01, verbose=False, num_iter=500, threshold = 0.4, L=3, N=[4,3,1], lambd=0.1)\n",
        "  nnr.fit(X,y)\n",
        "  print('The predicted value is::{}'.format(nnr.predict(np.array([10,12,23]))))\n",
        "  num_iter, JArray = nnr.array_error()\n",
        "  print(num_iter)\n",
        "\n",
        "  plt.plot(range(num_iter),JArray)\n",
        "  plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The predicted value is::0.9982109735328482\n",
            "500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyBqhQBcRWSMU6CIia4QCXURkjfj/zoB7mk0AZtkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOkfcktBo9PT",
        "colab_type": "code",
        "outputId": "9f3d4c75-4019-4259-b41b-a3427ffa8039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for i in range(1,3):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbCVID61oeY",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network with DropOut and Regularized Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-iyH9c5Kkr7",
        "colab_type": "code",
        "outputId": "bde53606-b5b5-4117-e65c-4ca267c5817c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Main Class \n",
        "\n",
        "class NeuralNetworkD:\n",
        "  #initialize the class with hyperparameters. Here L is the depth and N is the array \n",
        "  #that contains the no of node in each Depth\n",
        "  def __init__(self, alpha=0.01, num_iter = 25, verbose='False', threshold=0.0001, L=2, N=[4,1], lambd=1,dropout=[1,0.8,1,1]):\n",
        "    \n",
        "    #hyperparameter\n",
        "    self.alpha = alpha\n",
        "    self.num_iter = num_iter\n",
        "    self.verbose = verbose \n",
        "    self.threshold = threshold \n",
        "    self.L = L\n",
        "    self.N = N\n",
        "    self.lambd = lambd\n",
        "    self.dropout = dropout\n",
        "    \n",
        "    #parameter\n",
        "    self.nx = 0\n",
        "    self.m = 0\n",
        "    self.W = {}\n",
        "    self.B = {}\n",
        "    self.JArray = list()\n",
        "    self.d = {}\n",
        "    #variable for forward propagation\n",
        "    self.Z = {}\n",
        "    self.A = {}\n",
        "\n",
        "    #variable for backward propagation\n",
        "    self.dZ = {}\n",
        "    self.dW = {}\n",
        "    self.dB = {}\n",
        "    self.dA = {}\n",
        "\n",
        "  def __sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "   \n",
        "  #relu activation\n",
        "  def __relu(self,Z):\n",
        "    return np.maximum(0,Z)\n",
        "  \n",
        "  #relu derivative activation\n",
        "  def __reluDerivative(self,Z):\n",
        "       Z[Z<=0] = 0\n",
        "       Z[Z>0] = 1\n",
        "       return Z\n",
        "  \n",
        "  #predict the value for given input x\n",
        "  def predict(self,x):\n",
        "    A = {}\n",
        "    \n",
        "    A[0] = x\n",
        "    Z = {}\n",
        "    for i in range(1,self.L+1):\n",
        "        if(i<(self.L)):\n",
        "          Z[i] = np.dot(self.W[i], A[i-1])+ self.B[i]\n",
        "          A[i] = self.__relu(Z[i])\n",
        "        else:\n",
        "          Z[i] = np.dot(self.W[i],A[i-1])+self.B[i]\n",
        "          A[i] = self.__sigmoid(Z[i])\n",
        "    return A[self.L].mean()\n",
        "  \n",
        "  #initialize weights   \n",
        "  def __weightInitialize(self):\n",
        "    self.N.insert(0,self.nx)\n",
        "    \n",
        "    for i in range(1,self.L+1):\n",
        "      self.W[i] = np.random.randn(self.N[i],self.N[i-1]) * np.sqrt(1/self.N[i-1])\n",
        "      self.B[i] = np.random.randn(self.N[i],1)\n",
        "  \n",
        "  #return array of errors \n",
        "  def array_error(self):\n",
        "    return self.num_iter,self.JArray\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    self.nx = X.shape[0]\n",
        "    self.m = X.shape[1]\n",
        "\n",
        "    #initialize all the weights and bias\n",
        "    self.__weightInitialize()\n",
        "    self.A[0] = X\n",
        "    \n",
        "    #epoch start from here\n",
        "    for t in range(self.num_iter):\n",
        "\n",
        "      ##START OF FORWARD PROPAGATION\n",
        "      for i in range(1,self.L+1):\n",
        "        if(i<(self.L)):\n",
        "          self.Z[i] = np.dot(self.W[i], self.A[i-1])+ self.B[i]\n",
        "          self.A[i] = self.__relu(self.Z[i])\n",
        "          self.d[i] = np.random.rand(self.A[i].shape[0],self.A[i].shape[1])<self.dropout[i]\n",
        "          self.A[i] = (self.A[i] * self.d[i])/self.dropout[i]\n",
        "        else:\n",
        "          \n",
        "          self.Z[i] = np.dot(self.W[i],self.A[i-1])+self.B[i]\n",
        "          self.A[i] = self.__sigmoid(self.Z[i])  \n",
        "          self.d[i] = np.random.rand(self.A[i].shape[0],self.A[i].shape[1])<self.dropout[i]\n",
        "          self.A[i] = (self.A[i] * self.d[i])/self.dropout[i]\n",
        "      ##END OF FORWARD PROPAGATION\n",
        "      \n",
        "      #ERROR CALCULATION\n",
        "      j = - (y * np.log(self.A[self.L]) + (1-y) * np.log(1-self.A[self.L])) #error calculation\n",
        "      J = j.mean()\n",
        "      self.JArray.append(J)\n",
        "      if(self.verbose==True):\n",
        "        print(\"The error at this iteration {} is {}\".format(t,J))\n",
        "      if(J<self.threshold):\n",
        "        break\n",
        "\n",
        "      ##START OF BACKWARD PROPAGATION\n",
        "      self.dZ[self.L] = self.A[self.L] - y\n",
        "      self.dW[self.L] = ((1/self.m) * np.dot(self.dZ[self.L],self.A[self.L].T))\n",
        "      self.dB[self.L] = (1/self.m) * np.sum(self.dZ[self.L],axis=1,keepdims=True)\n",
        "      self.dA[self.L-1] = np.dot(self.W[self.L].T, self.dZ[self.L]) \n",
        "      for i in range(self.L-1,0,-1):\n",
        "        self.dZ[i] = self.dA[i] * self.__reluDerivative(self.Z[i])\n",
        "        self.dW[i] = (1/self.m) * np.dot(self.dZ[i],self.A[i-1].T)\n",
        "        self.dB[i] = (1/self.m) * np.sum(self.dZ[i],axis=1, keepdims=True)\n",
        "        self.dA[i-1] = np.dot(self.W[i].T,self.dZ[i])\n",
        "      ##END OF BACKWARD PROPAGATION\n",
        "\n",
        "      ##WEIGHT UPDATE START\n",
        "      for i in range(1,self.L+1):\n",
        "        self.W[i] = self.W[i] - self.alpha * self.dW[i]\n",
        "        self.B[i] = self.B[i] - self.alpha * self.dB[i]\n",
        "      ## WEIGHT UPDATE ENDS     \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  X = np.array([[1,1],\n",
        "                [2,2],\n",
        "                [3,3],\n",
        "                [4,4]])\n",
        "  X = X.T\n",
        "  y = np.array([21,44,69,816])\n",
        "\n",
        "  nnd = NeuralNetworkD(alpha=0.01, verbose=False, num_iter=500, threshold = 0.4, L=3, N=[4,3,1], lambd=0.1)\n",
        "  nnd.fit(X,y)\n",
        "  print('The predicted value is::{}'.format(nnd.predict(np.array([5,5]))))\n",
        "  # num_iter, JArray = nnd.array_error()\n",
        "  # print(num_iter)\n",
        "\n",
        "  # plt.plot(range(num_iter),JArray)\n",
        "  # plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The predicted value is::0.8817590447924107\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}