{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3KVWRQDYMKVn0TzZQzaSl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prd-dahal/AI_Projects/blob/master/Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdLYMg5Ltjgv",
        "colab_type": "text"
      },
      "source": [
        "## **Week 2 Practice**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi5xZxvutnIe",
        "colab_type": "code",
        "outputId": "f5f43c08-1fdf-4473-e5d0-102011642ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "#defination of sigmoid function\n",
        "def sigmoid(x):\n",
        "  return 1/(1+math.pow(2.71828,-x))\n",
        "\n",
        "#defination of output function where w is array of weight and x is array of inputs and b is the bias\n",
        "def output(w,x,b):\n",
        "  return sigmoid(((np.transpose(w)*x).sum()+b))\n",
        "\n",
        "#defination of log fuction as log 0 myst give -infinite value \n",
        "def log(x):\n",
        "    if(x==0):\n",
        "      return -(99999)\n",
        "    else:\n",
        "      return math.log(x)\n",
        "\n",
        "#defination of cost function where y and y_cap  are numpy arrays of dimension 1 * nx where nx is \n",
        "#the number of features\n",
        "def cost_function(y, y_cap):\n",
        "  m = len(y)\n",
        "  temp_y_cap = list()\n",
        "  temp_y_cap1 = list()\n",
        "  for i in y_cap:\n",
        "    temp_y_cap.append(log(i))\n",
        "    temp_y_cap1.append(log(1-i))\n",
        "  \n",
        "  temp_y_cap = np.array(temp_y_cap)\n",
        "  temp_y_cap1 = np.array(temp_y_cap1) \n",
        "  \n",
        "  temp_cost = (y*temp_y_cap)+ (1-y)*temp_y_cap1\n",
        "  temp_cost = -(temp_cost.sum())\n",
        "  return (-1/m)*temp_cost\n",
        "\n",
        "#main function exceution\n",
        "y = np.array([1, 0, 1])\n",
        "y_cap = np.array([1, 1, 1])\n",
        "print(cost_function(y,y_cap))\n",
        "\n",
        "w = np.array([[1],[2],[3]])\n",
        "x = np.array([[1, 2, 3]])\n",
        "b = 3\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33333.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaKfo95i7GHo",
        "colab_type": "text"
      },
      "source": [
        "# **Logistic Regression USING LOOPS ( i.e Without Vectorization**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_OdbeJLDxma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Following Code is not the efficient code but rather a make it work kind of code.\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random \n",
        "\n",
        "class LogisticRegression:\n",
        "  \n",
        "  def __init__(self,num_iter,alpha):\n",
        "    self.num_iter = num_iter\n",
        "    self.alpha = alpha\n",
        "    self.weights = list()\n",
        "    self.b = float()\n",
        "\n",
        "  #private function to calculate sigmoid\n",
        "  def __sigmoid(self,x):\n",
        "    return 1/(1+np.exp(x))\n",
        "  \n",
        "  #fit the object with the training data X and y\n",
        "  def fit(self,X,y):\n",
        "    nx = len(X[0]) # no of features\n",
        "    m = len(X)    #no of training examples\n",
        "\n",
        "    #intitialize all weights to zero\n",
        "    for i in range(nx):\n",
        "      self.weights.append(random.randint(-10,10))\n",
        "    self.b = random.randint(-10,10)\n",
        "\n",
        "    for t in range(self.num_iter): #loop unitil the number of interation provided by user\n",
        "      \n",
        "      #variable intialization\n",
        "      z = list()\n",
        "      a = list()\n",
        "      J = 0\n",
        "      dw = list()\n",
        "      db = 0\n",
        "      \n",
        "      for i in range(m):\n",
        "       \n",
        "        z.append(np.dot(X[i],self.weights) + self.b)\n",
        "        a.append(self.__sigmoid(z[i]))\n",
        "        J = J +  (- ((y[i]*np.log(a[i]))))\n",
        "        dz = a[i]-y[i]\n",
        "        \n",
        "        for j in range(nx):\n",
        "          if (i==0):\n",
        "            dw.append((X[i][j] * dz))\n",
        "          else:\n",
        "            dw[j] = dw[j] + (X[i][j] * dz)\n",
        "        db = db + a[i] - y[i]\n",
        "      \n",
        "      for i in range(nx):\n",
        "         dw[j] = dw[j]/m\n",
        "      \n",
        "      J = J/m\n",
        "      db = db/m\n",
        "      \n",
        "      for i in range(len(self.weights)):\n",
        "         self.weights[i] = self.weights[i] - self.alpha * dw[i] \n",
        "      \n",
        "\n",
        "#main function\n",
        "feature_x = [[1,2,3],\n",
        "             [3,4,5],\n",
        "             [6,7,8]]\n",
        "result_y = [1,0,1]\n",
        "\n",
        "lr = LogisticRegression(5,0.1)\n",
        "lr.fit(feature_x,result_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkVMiIWli1GP",
        "colab_type": "text"
      },
      "source": [
        "### Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MokKFtsi3Nr",
        "colab_type": "code",
        "outputId": "e36d4db4-8fa9-4aa6-c943-e00428a5ca16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import time \n",
        "a = np.random.rand(1000000)\n",
        "b = np.random.rand(1000000)\n",
        "\n",
        "tic = time.time()\n",
        "c = np.dot(a,b)\n",
        "print(c)\n",
        "toc = time.time()\n",
        "\n",
        "print(\"It took {}ms\".format(1000*(toc-tic)))\n",
        "\n",
        "sum=0\n",
        "tic1 = time.time()\n",
        "for i in range(len(a)):\n",
        "  sum = sum + a[i] * b[i]\n",
        "print(sum)\n",
        "toc1 = time.time()\n",
        "\n",
        "print(\"It took {}ms\".format(1000*(toc1-tic1)))\n",
        "\n",
        "\n",
        "print(\"Vectorization is {} times faster\".format((toc1-tic1)/(toc-tic)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250074.12538953198\n",
            "It took 1.734018325805664ms\n",
            "250074.12538952183\n",
            "It took 655.620813369751ms\n",
            "Vectorization is 378.09335899903755 times faster\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjDRXs6kw98u",
        "colab_type": "text"
      },
      "source": [
        "## Broadcasting in python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb5VoXRYmHJ3",
        "colab_type": "code",
        "outputId": "2bb5ac7f-3786-404f-a814-ec304c121848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[56.0,0.0,4.4,68.0],\n",
        "              [1.2,104.0,52.0,8.0],\n",
        "              [1.8,135.0,99.0,0.9]])\n",
        "# print(A)\n",
        "print(A.sum(axis=0))\n",
        "print(A.sum(axis=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 59.  239.  155.4  76.9]\n",
            "[128.4 165.2 236.7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6Br6tsFK3P3",
        "colab_type": "text"
      },
      "source": [
        "# **Logistic Regression Using Vectorization ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E6lQRTlLKI6",
        "colab_type": "code",
        "outputId": "ab1978b5-e9de-4748-bd91-4f3436f7c267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class VLogisticRegression:\n",
        "  def __init__(self, num_iter, alpha, verbose = 0,tolerance = 0.00000000000001):\n",
        "    self.num_iter = num_iter\n",
        "    self.alpha = alpha\n",
        "    self.weights = list()\n",
        "    self.intercept = 0\n",
        "    self.verbose = verbose\n",
        "    self.tolerance = tolerance\n",
        "    self.total_time = 999\n",
        "  \n",
        "  def __sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "  def weights_(self):\n",
        "    return self.weights #return array of weights \n",
        "  \n",
        "  def intercept_(self):\n",
        "    return self.intercept #return intercept/b value\n",
        "  \n",
        "  def time_to_fit(self):\n",
        "    return self.total_time\n",
        "  \n",
        "  def predict(self,x):\n",
        "    return self.__sigmoid(np.dot(self.weights,x) + self.intercept) #return the predicted value\n",
        "  \n",
        "  def fit(self,X,y): #fit function where X is the 2D matrix of size m*nx here m = no of training data and nx = no of feature in each  \n",
        "    starting_time = time.time()\n",
        "    nx = X.shape[1] \n",
        "    m = X.shape[0]\n",
        "    dw = np.random.randn(nx)\n",
        "    weights = np.random.randn(1,nx)\n",
        "    b = np.random.randn(1,1)\n",
        "    \n",
        "    for i in range(self.num_iter):  # gradient descent loop\n",
        "      Z = np.dot(weights, X.T) + b\n",
        "      a = 1/(1+np.exp(-Z))\n",
        "      dZ = a-y\n",
        "      j = - (y * np.log(a) + (1-y)* np.log(1-a)) #error calculation\n",
        "      J = np.mean(j)\n",
        "      db = np.mean(dZ)\n",
        "      dw = ((dZ.reshape(4,1) * X).sum(axis=0))/m\n",
        "      weights = weights - (self.alpha * dw)\n",
        "      b = b- self.alpha * db \n",
        "      if(self.verbose==1): #print the error in each iteration if verbose is set to 1\n",
        "        print(\"The error at the interation {} is {}\".format(i,J))\n",
        "      if(J<self.tolerance): #break the iteration if the error is less than tolerance\n",
        "        break;\n",
        "    self.weights = weights\n",
        "    self.intercept = b \n",
        "    \n",
        "    end_time = time.time()\n",
        "    self.total_time = (end_time - starting_time) * 1000\n",
        "\n",
        "#Main Function \n",
        "if __name__ == '__main__':\n",
        "  X = np.array([[1,2,3],\n",
        "                [4,5,6],\n",
        "                [8,9,10],\n",
        "                [11,12,13]])\n",
        "  y = np.array([1,0,1,1])\n",
        "  VLR = VLogisticRegression(100,0.1, verbose=1,tolerance=0.1)\n",
        "  VLR.fit(X,y)\n",
        "  print(\"Weights:{}\".format(VLR.weights_()))\n",
        "  print(\"Intercept:{}\".format(VLR.intercept_()))\n",
        "  print(\"Prediction:{}\".format(VLR.predict([4,5,6])))\n",
        "  print(\"Time:{}ms\".format(VLR.time_to_fit()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The error at the interation 0 is 1.0763708583537959\n",
            "The error at the interation 1 is 0.7225765347695396\n",
            "The error at the interation 2 is 0.5311821149901664\n",
            "The error at the interation 3 is 0.5278434808387594\n",
            "The error at the interation 4 is 0.5287024610809044\n",
            "The error at the interation 5 is 0.526564375078376\n",
            "The error at the interation 6 is 0.5271892694378818\n",
            "The error at the interation 7 is 0.5256293317434763\n",
            "The error at the interation 8 is 0.5261418175400572\n",
            "The error at the interation 9 is 0.5249037718886373\n",
            "The error at the interation 10 is 0.5253678366033077\n",
            "The error at the interation 11 is 0.5243230607252717\n",
            "The error at the interation 12 is 0.5247786672673566\n",
            "The error at the interation 13 is 0.5238530649772298\n",
            "The error at the interation 14 is 0.524329566742001\n",
            "The error at the interation 15 is 0.5234750274279005\n",
            "The error at the interation 16 is 0.523997548940603\n",
            "The error at the interation 17 is 0.5231787995479871\n",
            "The error at the interation 18 is 0.523771789095732\n",
            "The error at the interation 19 is 0.5229594149570164\n",
            "The error at the interation 20 is 0.5236489329840696\n",
            "The error at the interation 21 is 0.522815101309358\n",
            "The error at the interation 22 is 0.5236304586669944\n",
            "The error at the interation 23 is 0.5227458916003277\n",
            "The error at the interation 24 is 0.5237208333628105\n",
            "The error at the interation 25 is 0.5227524182653489\n",
            "The error at the interation 26 is 0.523925814825132\n",
            "The error at the interation 27 is 0.5228346843576772\n",
            "The error at the interation 28 is 0.5242505365669174\n",
            "The error at the interation 29 is 0.5229907642748559\n",
            "The error at the interation 30 is 0.5246972458258073\n",
            "The error at the interation 31 is 0.5232155529607092\n",
            "The error at the interation 32 is 0.5252628463359376\n",
            "The error at the interation 33 is 0.5234998540162691\n",
            "The error at the interation 34 is 0.5259367471175025\n",
            "The error at the interation 35 is 0.5238302047310324\n",
            "The error at the interation 36 is 0.5266998157735279\n",
            "The error at the interation 37 is 0.5241897667204806\n",
            "The error at the interation 38 is 0.5275252402691939\n",
            "The error at the interation 39 is 0.5245602932409624\n",
            "The error at the interation 40 is 0.5283816021612859\n",
            "The error at the interation 41 is 0.5249247090612107\n",
            "The error at the interation 42 is 0.529237536285515\n",
            "The error at the interation 43 is 0.5252694830328241\n",
            "The error at the interation 44 is 0.5300665031110903\n",
            "The error at the interation 45 is 0.5255860167897276\n",
            "The error at the interation 46 is 0.5308500592501432\n",
            "The error at the interation 47 is 0.5258707281965098\n",
            "The error at the interation 48 is 0.5315787478902246\n",
            "The error at the interation 49 is 0.526124074018881\n",
            "The error at the interation 50 is 0.5322508357498833\n",
            "The error at the interation 51 is 0.5263490837076763\n",
            "The error at the interation 52 is 0.5328698749228548\n",
            "The error at the interation 53 is 0.5265499529651012\n",
            "The error at the interation 54 is 0.5334421496077796\n",
            "The error at the interation 55 is 0.5267310104280167\n",
            "The error at the interation 56 is 0.5339746829618582\n",
            "The error at the interation 57 is 0.52689612318353\n",
            "The error at the interation 58 is 0.534474011166231\n",
            "The error at the interation 59 is 0.5270484571318299\n",
            "The error at the interation 60 is 0.53494562104797\n",
            "The error at the interation 61 is 0.5271904623955396\n",
            "The error at the interation 62 is 0.5353938319231463\n",
            "The error at the interation 63 is 0.5273239703061373\n",
            "The error at the interation 64 is 0.5358219136955574\n",
            "The error at the interation 65 is 0.5274503258330052\n",
            "The error at the interation 66 is 0.5362322942691398\n",
            "The error at the interation 67 is 0.5275705141797342\n",
            "The error at the interation 68 is 0.5366267722638909\n",
            "The error at the interation 69 is 0.5276852645575592\n",
            "The error at the interation 70 is 0.5370066972378092\n",
            "The error at the interation 71 is 0.527795127998284\n",
            "The error at the interation 72 is 0.5373731072288743\n",
            "The error at the interation 73 is 0.5279005324505872\n",
            "The error at the interation 74 is 0.5377268269698332\n",
            "The error at the interation 75 is 0.5280018203764303\n",
            "The error at the interation 76 is 0.5380685349936405\n",
            "The error at the interation 77 is 0.5280992738710522\n",
            "The error at the interation 78 is 0.5383988082293668\n",
            "The error at the interation 79 is 0.5281931313287372\n",
            "The error at the interation 80 is 0.5387181512348557\n",
            "The error at the interation 81 is 0.5282835985636709\n",
            "The error at the interation 82 is 0.5390270153433748\n",
            "The error at the interation 83 is 0.5283708563576694\n",
            "The error at the interation 84 is 0.539325811343914\n",
            "The error at the interation 85 is 0.528455065714562\n",
            "The error at the interation 86 is 0.5396149180579449\n",
            "The error at the interation 87 is 0.5285363716297249\n",
            "The error at the interation 88 is 0.5398946883057911\n",
            "The error at the interation 89 is 0.5286149058790605\n",
            "The error at the interation 90 is 0.5401654531892045\n",
            "The error at the interation 91 is 0.5286907891421534\n",
            "The error at the interation 92 is 0.5404275252622998\n",
            "The error at the interation 93 is 0.528764132658633\n",
            "The error at the interation 94 is 0.54068120094684\n",
            "The error at the interation 95 is 0.528835039546595\n",
            "The error at the interation 96 is 0.540926762417529\n",
            "The error at the interation 97 is 0.5289036058691264\n",
            "The error at the interation 98 is 0.5411644791042683\n",
            "The error at the interation 99 is 0.5289699215083536\n",
            "Weights:[[-0.40089649  0.56482848  0.15502943]]\n",
            "Intercept:[[-0.90421713]]\n",
            "Prediction:[[0.77669616]]\n",
            "Time:31.68964385986328ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVD-3IKPga9F",
        "colab_type": "code",
        "outputId": "2f00f64e-0509-483b-c54e-f2bfa7bc3440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## HIT AND TIRAL FOR THE ALOGIRTHM ABOVE\n",
        "\n",
        "X = np.array([[1,2,3],\n",
        "              [4,5,6],\n",
        "              [8,9,10],\n",
        "              [11,12,13]])\n",
        "y = np.array([1,0,1,1])\n",
        "# print(X.shape[1])\n",
        "\n",
        "weights = np.ones(3)\n",
        "# print(weights)\n",
        "b = np.ones(1)\n",
        "Z = (np.dot(weights,X.T)+b)\n",
        "\n",
        "a = 1/(1+np.exp(-Z))\n",
        "# print(Z)\n",
        "# print(a)\n",
        "# print(y)\n",
        "dZ = a-y\n",
        "dw = (dZ.reshape(4,1)) * X\n",
        "# print(dw)\n",
        "dw = (dw.sum(axis=0))\n",
        "# print(dw)\n",
        "# print(dw/3)\n",
        "# print(dz)\n",
        "print(weights)\n",
        "weights = weights - 0.1 * (dw)\n",
        "print(weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1.]\n",
            "[0.60009115 0.50018227 0.40027338]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KGa_HNr_GVo",
        "colab_type": "text"
      },
      "source": [
        "## 2 Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JBnMV-t_Ma-",
        "colab_type": "code",
        "outputId": "ec5c6243-b02c-40f6-8c27-1776eea3bcf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "  return 1/ (1+np.exp(-x)) \n",
        "n_node = 4 \n",
        "nx = 3 \n",
        "feature_x = np.array([[2,3,4],\n",
        "                      [3,4,5],\n",
        "                      [7,8,9]])\n",
        "W1 = np.random.randn(4,3)\n",
        "print(W1)\n",
        "\n",
        "feature_x = feature_x.T\n",
        "print(feature_x)\n",
        "Z =np.dot(W1  , feature_x) \n",
        "print(Z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.53127221  0.91732151 -0.55024089]\n",
            " [ 0.13927189  3.00293509  2.11772257]\n",
            " [-1.56630397  1.34943927 -0.08199806]\n",
            " [-0.63398165 -0.62512628 -1.80590942]]\n",
            "[[2 3 7]\n",
            " [3 4 8]\n",
            " [4 5 9]]\n",
            "[[ -2.51154342  -3.675735    -8.33250131]\n",
            " [ 17.75823932  23.01816887  44.05788707]\n",
            " [  0.58771765   0.28885489  -0.90659614]\n",
            " [-10.36697981 -13.43199716 -25.69206654]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlStGMrQPneo",
        "colab_type": "text"
      },
      "source": [
        "# Multi Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsQJDBp6PsIs",
        "colab_type": "code",
        "outputId": "14dd5ea2-d6f3-4ef9-af4f-7d0cbe769290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self,no_of_node1, no_of_final_node, alpha, num_iter,verbose=0,threshold=0.00000001):\n",
        "    self.no_of_node1 = no_of_node1\n",
        "    self.no_of_final_node = no_of_final_node\n",
        "    self.alpha = alpha\n",
        "    self.threshold = threshold\n",
        "    self.num_iter = num_iter\n",
        "    self.W1 = list()\n",
        "    self.b1 = list()\n",
        "    self.W2 = list()\n",
        "    self.b2 = list()\n",
        "    self.m = 0\n",
        "    self.nx = 0\n",
        "    self.verbose = verbose\n",
        "    \n",
        "  #relu activation\n",
        "  def __relu(self,Z):\n",
        "    return np.maximum(0,Z)\n",
        "  \n",
        "  #sigmoid activation\n",
        "  def __sigmoid(self,Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "  \n",
        "  #relu derivative activation\n",
        "  def __reluDerivative(self,Z):\n",
        "       Z[Z<=0] = 0\n",
        "       Z[Z>0] = 1\n",
        "       return Z\n",
        "  #initialize the weights\n",
        "  def __initialize_weight(self):\n",
        "    self.W1 = np.random.randn(self.no_of_node1,self.nx) * 0.01\n",
        "    self.b1 = np.random.randn(self.no_of_node1,1)\n",
        "    self.W2 = np.random.randn(self.no_of_final_node, self.no_of_node1) * 0.01\n",
        "    self.b2 = np.random.randn(self.no_of_final_node,1)\n",
        "  \n",
        "  def predict(self,X):\n",
        "    Z1 = np.dot(self.W1,X) + self.b1\n",
        "    \n",
        "    A1 = self.__relu(Z1)\n",
        "    Z2 = np.dot(self.W2,A1) + self.b2\n",
        "    A2 = self.__sigmoid(Z2) \n",
        "    return A2.mean()\n",
        "  def fit(self,X,y):\n",
        "    self.nx = X.shape[0]\n",
        "    self.m = X.shape[1]\n",
        "\n",
        "    #initialize Weight\n",
        "    self.__initialize_weight()\n",
        "    for i in range(self.num_iter):\n",
        "      ## START of forward Propagation\n",
        "      Z1 = np.dot(self.W1,X) + self.b1\n",
        "      \n",
        "      #use of activation function\n",
        "      A1 = self.__relu(Z1)\n",
        "      \n",
        "      #second layer calculation\n",
        "      Z2 = np.dot(self.W2, A1) + self.b2\n",
        "      A2 = self.__sigmoid(Z2) #output\n",
        "      \n",
        "      ## END of Forward Calculation\n",
        "\n",
        "      #ERROR CALCULATION\n",
        "      j = - (y * np.log(A2) + (1-y)* np.log(1-A2)) #error calculation\n",
        "      J = j.mean()\n",
        "      if(self.verbose==True):\n",
        "        print(\"The error at this iteration {} is {}\".format(i,J))\n",
        "      if(J<self.threshold):\n",
        "        break\n",
        "      ##START OF BACKWARD CALCULATION\n",
        "      dZ2 = A2-y\n",
        "      dW2 = (1/self.m) * np.dot(dZ2 , A1.T)\n",
        "      db2 = (1/self.m)* np.sum(dZ2,axis=1,)\n",
        "      dZ1 = np.dot(self.W2.T,dZ2) * (self.__reluDerivative(Z1))\n",
        "      dW1 = (1/self.m) * np.dot(dZ1,X.T)\n",
        "      db1 = (1/self.m) * np.sum(dZ1, axis=1,keepdims=True)\n",
        "      #END OF BACKWARD CALCULATOIN\n",
        "\n",
        "      ## Update Weights\n",
        "      self.W1 = self.W1 - self.alpha * dW1\n",
        "      self.b1 = self.b1 - self.alpha * db1\n",
        "      self.W2 = self.W2 - self.alpha * dW2\n",
        "      self.b2 = self.b2 - self.alpha * db2\n",
        "\n",
        "X = np.array([[1,2,3],\n",
        "              [4,5,6],\n",
        "              [8,9,10],\n",
        "              [1,5,1],\n",
        "              [2,3,4]])\n",
        "X = X.T\n",
        "\n",
        "y = np.array([1,0,1,1,1])\n",
        "\n",
        "nn = NeuralNetwork(4,1,0.01,500, verbose=False,threshold=0.4)\n",
        "nn.fit(X,y)\n",
        "\n",
        "test = np.array([3,4,5])\n",
        "predicted = nn.predict(test)\n",
        "print(predicted)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8058638708689416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36b9fjY6RKLI",
        "colab_type": "code",
        "outputId": "735e1ab3-92b7-417b-a545-7bb099c2859b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import numpy as np\n",
        "X = np.array([[1,2,3],\n",
        "              [4,5,6],\n",
        "              [8,9,10],\n",
        "              [11,12,13]])\n",
        "X = X.T\n",
        "\n",
        "y = np.array([1,0,1,1])\n",
        "X.shape[0]\n",
        "W1= np.ones((4,3))\n",
        "b1 = np.ones((4,1))\n",
        "W2 = np.ones((1, 4))\n",
        "b2 = np.ones((1,1))\n",
        "\n",
        "\n",
        "Z1 = (np.dot(W1,X))+b1\n",
        "\n",
        "def relu(X):\n",
        "    return np.maximum(0,X)\n",
        "\n",
        "\n",
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "# print(Z1)\n",
        "A1= relu(Z1)\n",
        "\n",
        "\n",
        "Z2 = np.dot(W2,A1) + b2\n",
        "A2 = sigmoid(Z2)\n",
        "j = - (y * np.log(A2) + (1-y)* np.log(1-A2)) #error calculation\n",
        "\n",
        "J = j.mean()\n",
        "\n",
        "dZ2 = np.dot(A2,y)\n",
        "\n",
        "dW2 = (1/4)*dZ2 *  A1.T\n",
        "def reluDerivative(Z):\n",
        "       Z[Z<=0] = 0\n",
        "       Z[Z>0] = 1\n",
        "       return Z\n",
        "print(reluDerivative(Z1))  \n",
        "dZ1 = np.dot(W2.T,dZ2) * reluDerivative(Z1)\n",
        "\n",
        "\n",
        "dW1 = (1/4) * np.dot(dZ1,X.T)\n",
        "\n",
        "db1 = (1/4) * np.sum(dZ1, axis=1,keepdims=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}